# -*- coding: utf-8 -*-
"""EF308 - Group Assignment.pynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kdj2pTYgRTzntH7eYTTheqUoL4FED37U
"""

import numpy as np
import pandas as pd

# Import visualisation packages

import matplotlib.pyplot as plt
import plotly.express as px

# import statistics package

import statsmodels.api as sm
import statsmodels.tsa.api as smt   # new
import statsmodels.tsa.stattools as st

import scipy.stats as stats

"""# **Import Dataset**"""

creditdata = pd.read_csv('BankChurners.csv')
creditdata

from google.colab import drive
drive.mount('/content/drive')

"""# **Information from Dataset**"""

creditdata.describe()

creditdata.info()

"""# *Data Cleaning*

# **Check for Null Values**
"""

# Handling missing / NaN Values

(creditdata.isna().sum(axis=0))
print(creditdata.isna().sum(axis=0)/len(creditdata))

"""Shows no null values in dataset

# **Check for Duplicates**
"""

# Check for duplicates
duplicate_rows = creditdata[creditdata.duplicated()]

# Print duplicate rows
print("Duplicate rows:")
print(duplicate_rows)

print("There are " + str(creditdata.duplicated().sum()) + " duplicated rows")

"""Illustrates no duplicates in our dataset

# **Checking Distribution of Data**
"""

mean = creditdata.mean()
print ('Mean:')
print(mean)

mode = creditdata.mode()
print ('\nMode:')
print (mode)

median = creditdata.median()
print ('\nMedian:')
print(median)

# Investigate Distribution

# Calculate skewness for all columns
skewness = creditdata.skew()
print ('Skewness:')
print(skewness)

# Calculate kurtosis for all columns
kurtosis = creditdata.kurtosis()
print ('\nKurtosis:')
print(kurtosis)

fig = px.histogram(creditdata, x="Total_Trans_Amt")
fig.show()

# Variance

creditdata['Credit_Limit'].var()

# Plotting distributions to see if there is actually positive skewness and low kurtosis

fig = px.histogram(creditdata, x="Credit_Limit")
fig.show()

"""We notice the data is not normally distributed based on the summary statistics from the mean, median, mode, standard deviation, skewness, and kurtosis.

We further illustrate this using the Shapiro test from the SciPy library (scipy.stats.shapiro) to check for normality.
"""

# Statistical Test

shapiro_test_stat, shapiro_p_value = stats.shapiro(creditdata)
print(f'Shapiro-Wilk Test - Test Statistic: {shapiro_test_stat}, p-value: {shapiro_p_value}')

# Interpret the result
if shapiro_p_value > 0.05:
    print("The data may be approximately normally distributed.")
else:
    print("The data may not be normally distributed.")

"""Due to large data set - the test was run on the head and tail sample of the dataset

Results shows the data is not normally distributed
"""

# Statistical Test
# Check distribution of data

shapiro_test_stat, shapiro_p_value = stats.shapiro(creditdata.head())
print(f'Shapiro-Wilk Test - Test Statistic: {shapiro_test_stat}, p-value: {shapiro_p_value}')

# Interpret the result
if shapiro_p_value > 0.05:
    print("The data may be approximately normally distributed.")
else:
    print("The data may not be normally distributed.")

# Statistical Test
# Check distribution of data

shapiro_test_stat, shapiro_p_value = stats.shapiro(creditdata.tail())
print(f'Shapiro-Wilk Test - Test Statistic: {shapiro_test_stat}, p-value: {shapiro_p_value}')

# Interpret the result
if shapiro_p_value > 0.05:
    print("The data may be approximately normally distributed.")
else:
    print("The data may not be normally distributed.")

"""# **Identifying Outliers**

### Using IQR Method

The IQR Method will be used as the data is not normally distributed.
"""

# Find quantiles

creditdata.quantile([.1, .25, .5, .75])

"""### Identifying Columns with Outliers"""

# Select numerical columns (assuming they are of type int or float)
numeric_cols = creditdata.select_dtypes(include=['int', 'float'])

# Calculate the first quartile (Q1)
Q1 = numeric_cols.quantile(0.25)

# Calculate the third quartile (Q3)
Q3 = numeric_cols.quantile(0.75)

# Calculate the interquartile range (IQR)
IQR = Q3 - Q1

# Identify outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Data points outside the lower and upper bounds are considered outliers
outliers = (numeric_cols < lower_bound) | (numeric_cols > upper_bound)

# Print columns with outliers
columns_with_outliers = outliers.any()
print("Columns with outliers:")
print(columns_with_outliers[columns_with_outliers])

"""### Visualisation of IQR

We can use the visuals below to investigate and visually see the outliers for each column.
"""

# Interquartile range

fig = px.box(creditdata, y="Months_on_book")
fig.show()

# Note:
# Upper and lower fences cordon off outliers from the bulk of data in a set.
# Fences are usually found with the following formulas:

# Upper fence = Q3 + (1.5 * IQR)
# Lower fence = Q1 â€” (1.5 * IQR).

# We can visualise where these outliers may appear on this graph

"""Upon review, we have determined these outliers to be true outliers, and hence we have not removed them from our dataset. \

# **Convert datatype from 'Object' to 'Int' to run regressions**
"""

# Verify the data types of the columns
print(creditdata.dtypes)

# Finding unique counts

creditdata.nunique()

# Display the value of each unique datapoint

creditdata['Gender'].unique()

creditdata['Attrition_Flag'].unique()

creditdata['Education_Level'].unique()

creditdata['Marital_Status'].unique()

creditdata['Income_Category'].unique()

creditdata['Card_Category'].unique()

# Gender

# Define a dictionary to map 'M' to 0 and 'F' to 1
gender_mapping = {'M': 0, 'F': 1}

# Map 'M' and 'F' values to integers
creditdata['Gender'] = creditdata['Gender'].map(gender_mapping)

# Verify the result
print(creditdata['Gender'])


# MALE = 0
# FEMALE = 1

# Attrition_Flag

attrition_mapping = {'Existing Customer': 0, 'Attrited Customer': 1}

creditdata['Attrition_Flag'] = creditdata['Attrition_Flag'].map(attrition_mapping)

print(creditdata['Attrition_Flag'])


# Existing customer = 0
# Attrited customer = 1

# Education_Level

education_mapping = {'High School': 0, 'Graduate': 1, 'Uneducated': 2, 'Unknown': 3, 'College': 4, 'Post-Graduate': 5, 'Doctorate': 6}

creditdata['Education_Level'] = creditdata['Education_Level'].map(education_mapping)

print(creditdata['Education_Level'])


# High School = 0
# Graduate = 1
# Uneducated = 2
# Unknown = 3
# College = 4
# Post-Graduate = 5
# Doctorate = 6

# Marital_Status

marital_mapping = {'Married': 0, 'Single': 1, 'Unknown': 2, 'Divorced': 3}

creditdata['Marital_Status'] = creditdata['Marital_Status'].map(marital_mapping)

print(creditdata['Marital_Status'])


# Married = 0
# Single = 1
# Unknown = 2
# Divorced = 3

# Card_Category

card_mapping = {'Blue': 0, 'Gold': 1, 'Silver': 2, 'Platinum': 3}

creditdata['Card_Category'] = creditdata['Card_Category'].map(card_mapping)

print(creditdata['Card_Category'])


# Blue = 0
# Gold = 1
# Silver = 2
# Platinum = 3

# Income_Category

income_mapping = {'Less than $40K': 2500, '$40K - $60K': 4166, '$60K - $80K': 5833, '$80K - $120K': 8333, '$120K +': 11666, 'Unknown': 0}

creditdata['Income_Category'] = creditdata['Income_Category'].map(income_mapping)

print(creditdata['Income_Category'])

# Less than $40K = 0
# $40K - $60K = 1
# $60K - $80K = 2
# $80K - $120K = 3
# $120K+ = 4
# Unknown = 5

creditdata

# shows updated table

creditdata.info()

# shows datatype is now int64

"""# Remove unnecessary columns"""

# List of columns to remove
columns_to_remove = [
    "Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1",
    "Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2",
    "Attrition_Flag",
    "Dependent_count",
    "Marital_Status",
    "Education_Level",
    "Months_on_book",
    "Total_Relationship_Count",
    "Months_Inactive_12_mon",
    "Contacts_Count_12_mon",
    "Credit_Limit",
    "Total_Revolving_Bal",
    "Avg_Open_To_Buy",
    "Total_Amt_Chng_Q4_Q1",
    "Total_Ct_Chng_Q4_Q1",
    "Avg_Utilization_Ratio"
]

# Removing the columns
creditdata = creditdata.drop(columns=columns_to_remove)

# Displaying the first rows of the DataFrame for verification
print(creditdata.head())

creditdata

# @title Income_Category

from matplotlib import pyplot as plt
import seaborn as sns
creditdata.groupby('Income_Category').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

"""# Creation of a new column to show total round-up per month, with 50 centimes as the average rounding."""

creditdata['Roundup_Amt'] = creditdata['Total_Trans_Ct'] * 0.5
print(creditdata.head())

"""# Creation of a new column showing the total commissions Revolut takes per customer thanks to the round-up."""

creditdata['Comission'] = creditdata['Roundup_Amt'] * 0.0025
print(creditdata.head())

total_comission = creditdata['Comission'].sum()

print("Total Comission:", total_comission)

"""# Regression to identify links between data"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Assuming creditdata is your DataFrame and contains the columns 'Income_Category' and 'Total_Trans_Ct'
# If Income_Category is not already numeric, you should convert it or use the mapping we previously discussed.

# Splitting the data into independent (X) and dependent (y) variables
X = creditdata[['Income_Category']]  # X is capitalized because it's a DataFrame
y = creditdata['Total_Trans_Ct']     # y is lowercase because it's a Series

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Creating a LinearRegression instance
model = LinearRegression()

# Training the model
model.fit(X_train, y_train)

# Making predictions on the test set
y_pred = model.predict(X_test)

# Model evaluation
print(f"Coefficient: {model.coef_}")
print(f"Intercept: {model.intercept_}")
print(f"Mean squared error: {mean_squared_error(y_test, y_pred)}")
print(f"Coefficient of determination (R^2): {r2_score(y_test, y_pred)}")

import matplotlib.pyplot as plt

# Plotting the data points
plt.scatter(X_test, y_test, color='black', label='Actual Data')

# Plotting the regression line
plt.plot(X_test, y_pred, color='blue', linewidth=3, label='Regression Line')

# Adding titles and legends
plt.title('Linear Regression between Income_Category and Total_Trans_Ct')
plt.xlabel('Income_Category')
plt.ylabel('Total_Trans_Ct')
plt.legend()

# Displaying the plot
plt.show()

# Splitting the data into independent (X) and dependent (y) variables
X = creditdata[['Total_Trans_Ct']]  # Independent variable
y = creditdata['Total_Trans_Amt']    # Dependent variable

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Creating and training the model
model = LinearRegression()
model.fit(X_train, y_train)

# Making predictions on the test set
y_pred = model.predict(X_test)

# Model evaluation
print(f"Coefficient: {model.coef_}")
print(f"Intercept: {model.intercept_}")
print(f"Mean squared error: {mean_squared_error(y_test, y_pred)}")
print(f"Coefficient of determination (R^2): {r2_score(y_test, y_pred)}")
# Plotting the data points
plt.scatter(X_test, y_test, color='black', label='Actual Data')

# Plotting the regression line
plt.plot(X_test, y_pred, color='blue', linewidth=3, label='Regression Line')

# Adding titles and legends
plt.title('Linear Regression between Total_Trans_Ct and Total_Trans_Amt')
plt.xlabel('Total_Trans_Ct')
plt.ylabel('Total_Trans_Amt')
plt.legend()

# Displaying the plot
plt.show()

# Splitting the data into independent (X) and dependent (y) variables
X = creditdata[['Income_Category']]  # Independent variable
y = creditdata['Total_Trans_Amt']    # Dependent variable

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Creating and training the model
model = LinearRegression()
model.fit(X_train, y_train)

# Making predictions on the test set
y_pred = model.predict(X_test)

# Model evaluation
print(f"Coefficient: {model.coef_}")
print(f"Intercept: {model.intercept_}")
print(f"Mean squared error: {mean_squared_error(y_test, y_pred)}")
print(f"Coefficient of determination (R^2): {r2_score(y_test, y_pred)}")
# Plotting the data points
plt.scatter(X_test, y_test, color='black', label='Actual Data')

# Plotting the regression line
plt.plot(X_test, y_pred, color='blue', linewidth=3, label='Regression Line')

# Adding titles and legends
plt.title('Linear Regression between Income_Category and Total_Trans_Amt')
plt.xlabel('Income_Category')
plt.ylabel('Total_Trans_Amt')
plt.legend()

# Displaying the plot
plt.show()

"""the most important regression, as it will enable us to forecast how much money revolut can make based on the number of transactions."""

# Independent variable
X = creditdata[['Total_Trans_Ct']]
# Dependent variable
y = creditdata['Comission']

# Creating and training the model
model = LinearRegression()
model.fit(X, y)

# Making predictions
y_pred = model.predict(X)

# Calculating statistics
print(f"Coefficient: {model.coef_[0]}")
print(f"Intercept: {model.intercept_}")
mse = mean_squared_error(y, y_pred)
print(f"Mean squared error: {mse}")
r2 = r2_score(y, y_pred)
print(f"Coefficient of determination (R^2): {r2}")

# Plotting
plt.scatter(X, y, color='black', label='Actual Data')
plt.plot(X, y_pred, color='blue', linewidth=3, label='Regression Line')
plt.title('Linear Regression between Total_Trans_Ct and Comission')
plt.xlabel('Total_Trans_Ct')
plt.ylabel('Comission')
plt.legend()
plt.show()

creditdata.to_csv('nouveau_creditdata.csv', index=False)

from google.colab import files
files.download('nouveau_creditdata.csv')